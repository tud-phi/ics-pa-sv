{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac408b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a96d02d6346fa7399cf6da0111ce0937",
     "grade": false,
     "grade_id": "cell-b00828259c8e42e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# RO47019: Intelligent Control Systems Practical Assignment\n",
    "* Period: 2022-2023, Q3\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/500969\n",
    "* Instructor: Cosimo Della Santina (C.DellaSantina@tudelft.nl)\n",
    "* Teaching assistant: Ruben Martin Rodriguez (R.MartinRodriguez@student.tudelft.nl)\n",
    "* (c) TU Delft, 2023\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Remove `raise NotImplementedError()` afterwards. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names, student numbers, netID, and emails below.\n",
    "STUDENT_1_NAME = \"\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"\"\n",
    "STUDENT_1_NETID = \"\"\n",
    "STUDENT_1_EMAIL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba32571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042927213b84aa368aa3ea72caa4cb60",
     "grade": true,
     "grade_id": "cell-9f148ec62e0de49c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert STUDENT_1_NAME != \"\"\n",
    "assert STUDENT_1_STUDENT_NUMBER != \"\"\n",
    "assert STUDENT_1_NETID != \"\"\n",
    "assert STUDENT_1_EMAIL != \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76ef40fcc3f08a0484661497162a1a9",
     "grade": false,
     "grade_id": "cell-4ea391677951116c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled Q&A hours to ask the TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGJrGKEVq2Gm"
   },
   "source": [
    "# SNN warm-up\n",
    "\n",
    "We will build a toy example in this section, aiming to generally master how to build a spiking neuron model, a simple network structure, and how the spikes propagate. Several frameworks exist to implement a spiking neural network (SNN), like `PySNN` or `SpikingJelly`. We adopt `snnTorch` in this practical assignment.\n",
    "\n",
    "snnTorch is a popular Python package to provide a well-embedded gradient-based learning framework for implementing SNNs. It extends the capabilities of PyTorch and could utilize GPU for tensor computation like the traditional neural networks. In addition, it also provides some currently popular neuron models, such as the LIF model. Fundamental neuron dynamics such as leakage have been encapsulated, and we can use them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn4lm0uSrBF_"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edzgy_I5jGYK"
   },
   "source": [
    "Here are some preparation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Csdlc8lar7Gd"
   },
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import surrogate\n",
    "import random\n",
    "\n",
    "# Some utils methods\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "\n",
    "# plot the membrane potential\n",
    "def print_mem(mem_record, title):\n",
    "    x_major_locator = MultipleLocator(1)\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title(title)\n",
    "    mem_record = np.array(mem_record)\n",
    "    plt.plot(mem_record)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Membrane Potential\")\n",
    "    plt.xlim([0, 20])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.gca().xaxis.set_major_locator(x_major_locator)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_spk(spk, title):\n",
    "    x_major_locator = MultipleLocator(1)\n",
    "    plt.figure(figsize=(10, 0.8))\n",
    "    plt.title(title)\n",
    "    splt.raster(spk, plt, s=400, c=\"black\", marker=\"|\")\n",
    "    plt.yticks([])\n",
    "    plt.xlim([0, spk.size()[0]])\n",
    "    plt.gca().xaxis.set_major_locator(x_major_locator)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_spk_mem(mem_record, spk_record, stimulus, threshold, title):\n",
    "    fig, ax = plt.subplots(\n",
    "        3, figsize=(10, 6), sharex=True, gridspec_kw={\"height_ratios\": [0.2, 1, 0.2]}\n",
    "    )\n",
    "    x_major_locator = MultipleLocator(1)\n",
    "\n",
    "    ax[0].set_title(title)\n",
    "    splt.raster(spk_record, ax[0], s=400, c=\"black\", marker=\"|\")\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_ylabel(\"Output spike\")\n",
    "\n",
    "    ax[1].plot(mem_record)\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    ax[1].set_ylabel(\"Membrane potential\")\n",
    "    ax[1].axhline(threshold, color=\"r\", linestyle=\"--\")\n",
    "    ax[1].grid()\n",
    "\n",
    "    splt.raster(stimulus, ax[2], s=400, c=\"black\", marker=\"|\")\n",
    "    ax[2].set_yticks([])\n",
    "    ax[2].set_ylabel(\"Input stimulus\")\n",
    "\n",
    "    plt.xlim([0, 20])\n",
    "    plt.gca().xaxis.set_major_locator(x_major_locator)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_network_loss(loss):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Training process\")\n",
    "    plt.plot(loss)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hty9N3fXrRc8"
   },
   "source": [
    "## Neuron Model: Leaky Integrate-and-Fire (LIF) Model\n",
    "\n",
    "We recommend using the LIF model for practice because its simplicity makes the training process more efficient while maintaining neuronal dynamics. In snnTorch, we could build a default LIF neuron like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39L-u4Rhrd21"
   },
   "outputs": [],
   "source": [
    "lif = snn.Leaky(beta=0.5)  # beta is the decay rate, which will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwThjrPuq1aW"
   },
   "source": [
    "The LIF model cares about three key features of the neuron membrane: leakage, accumulation, and threshold excitation. The current membrane potential is related to the previous potential (leakage process) and input stimulus (accumulation process). Therefore, if we have a stimulus that lasts three time steps, the leakage and accumulation process is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj3j8tRYrpZl"
   },
   "outputs": [],
   "source": [
    "# set the time steps\n",
    "time_steps = 20\n",
    "\n",
    "# create a stimulus\n",
    "stimulus = torch.zeros(time_steps)\n",
    "stimulus[5] = torch.tensor(0.2)\n",
    "stimulus[6] = torch.tensor(0.2)\n",
    "\n",
    "# initialize the LIF neuron before using it\n",
    "mem = lif.init_leaky()  # get the initialized membrane potential\n",
    "\n",
    "# feed the stimulus into the neuron in a recurrent way\n",
    "mem_record = []\n",
    "for t in range(time_steps):\n",
    "    spk_out, mem = lif(stimulus[t], mem)\n",
    "    mem_record.append(mem)\n",
    "\n",
    "mem_record = torch.stack(mem_record)\n",
    "print_mem(mem_record.detach(), \"neuron membrane potential leakage and accumulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEqxQeKXdPyP"
   },
   "source": [
    "At time step 5, the size of the input stimulus is 0.2, which is accumulated. So the membrane potential at time step 5 is 0.2. At time step 6, the neuron also receives a stimulus with a size of 0.2, which is accumulate, too. Due to the leakage feature of neurons, the potential at time step 5 leaks through the next time step at a decay rate of 0.5, so the final potential at time step 6 is (0.2 + 0.2*0.5)=0.3. In the next time steps, there is no more input stimulus. The membrane potential decays to 0 at the decay rate of 0.5. (We default 0 to be the resting potential.)\n",
    "\n",
    "The decay rate of a neuron is determined when the neuron is initialized. We set it with `beta` parameter. Next is the comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNY4cK-prtX0"
   },
   "outputs": [],
   "source": [
    "lif1 = snn.Leaky(beta=0.2)\n",
    "lif2 = snn.Leaky(beta=0.8)\n",
    "\n",
    "mem1 = lif1.init_leaky()\n",
    "mem2 = lif2.init_leaky()\n",
    "\n",
    "mem1_record = []\n",
    "mem2_record = []\n",
    "for t in range(time_steps):\n",
    "    spk_out1, mem1 = lif1(stimulus[t], mem1)\n",
    "    spk_out2, mem2 = lif2(stimulus[t], mem2)\n",
    "    mem1_record.append(mem1)\n",
    "    mem2_record.append(mem2)\n",
    "mem1_record = torch.stack(mem1_record)\n",
    "mem2_record = torch.stack(mem2_record)\n",
    "\n",
    "print_mem(mem1_record.detach(), \"neuron1: decay rate 0.2\")\n",
    "print_mem(mem2_record.detach(), \"neuron2: decay rate 0.8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJjLli81jutu"
   },
   "source": [
    "As for now, our neurons have biodynamics of accumulation and leakage. How about the threshold excitation? We could use the parameter `threshold` to set the neuron's threshold in snnTorch. The default threshold is 1. An example of a threshold excitation process is like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyPfV5dIiS_u"
   },
   "outputs": [],
   "source": [
    "stimulus = torch.zeros(time_steps)\n",
    "stimulus[3] = torch.tensor(0.5)\n",
    "stimulus[4] = torch.tensor(0.5)\n",
    "stimulus[7] = torch.tensor(0.5)\n",
    "stimulus[9] = torch.tensor(0.5)\n",
    "stimulus[13] = torch.tensor(0.5)\n",
    "stimulus[14] = torch.tensor(0.5)\n",
    "stimulus[15] = torch.tensor(0.5)\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "lif = snn.Leaky(beta=0.5, threshold=threshold)\n",
    "mem = lif.init_leaky()\n",
    "\n",
    "mem_record = []\n",
    "spk_record = []\n",
    "for t in range(time_steps):\n",
    "    spk_out, mem = lif(stimulus[t], mem)\n",
    "    mem_record.append(mem)\n",
    "    spk_record.append(spk_out)\n",
    "mem_record = torch.stack(mem_record)\n",
    "spk_record = torch.stack(spk_record)\n",
    "\n",
    "\n",
    "print_spk_mem(\n",
    "    mem_record.detach(),\n",
    "    spk_record.detach(),\n",
    "    stimulus,\n",
    "    threshold,\n",
    "    \"Example: threshold excitation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQEAKUfd1ITJ"
   },
   "source": [
    "Could you analyze this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu83R7ap1hXG"
   },
   "source": [
    "## A Simple Fully Connected SNN\n",
    "\n",
    "We have successfully built a LIF neuron using snnTorch. Then, how to connect those neurons into a network and make the gradient propagate through it? We will adopt a fully connected network structure as an example to show how to construct an SNN.\n",
    "\n",
    "At the beginning, we randomly generate a set of training data. It is worth noting that since snnTorch can use CUDA to calculate tensor, the format of the input data is usually `[time_steps, batch_size, _]`. We only generate a single event-based data in this example, and the data size is `[time_steps, 1]`, so we also need to encapsulate it into a batch form, that is, `[time_steps, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsMgAvUWmKCZ"
   },
   "outputs": [],
   "source": [
    "input = torch.tensor([random.randint(0, 1) for _ in range(time_steps)])\n",
    "label = torch.tensor(8)\n",
    "print_spk(input, \"Input spikes\")\n",
    "print(f\"The output label: {label}\")\n",
    "input = input.unsqueeze(1).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVNEGSMaBC29"
   },
   "source": [
    "Then, we build the network structure. snnTorch is a Pytorch-based framework so that it can extend all essential functions of PyTorch. We use snntorch to build neurons for the biodynamics of SNNs. For the change of synaptic weights, we still use the interface of PyTorch.\n",
    "\n",
    "We use the surrogate function instead of step functions as an activation mechanism for neurons. It is also determined when the neuron is initialized. In snnTorch, we could use `spike_grad` as the parameter to control the type of surrogate functions. In this example, we adopt the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iG9LOtYw5fzG"
   },
   "outputs": [],
   "source": [
    "class snnFullyConnectedModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, hidden_size, output_size, beta=0.8, time_steps=time_steps\n",
    "    ):\n",
    "        super().__init__()\n",
    "        spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        mem2_record = []\n",
    "\n",
    "        # deal with a data in the recurrent way\n",
    "        for t in range(self.time_steps):\n",
    "            current1 = self.fc1(x[t])\n",
    "            spk1, mem1 = self.lif1(current1, mem1)\n",
    "            current2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(current2, mem2)\n",
    "            mem2_record.append(mem2)\n",
    "\n",
    "        return torch.stack(mem2_record, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbPtg3-kP2aV"
   },
   "source": [
    "Since the neuron's threshold is 1, in the network's last layer, we use the membrane potential instead of spikes as the output. For some specific scenarios (such as classification problems, ground truth is also in pulse form), directly using membrane potential as output can provide more convenience for backpropagation while achieving the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK_ROzY5P4I8"
   },
   "outputs": [],
   "source": [
    "model = snnFullyConnectedModel(1, 5, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for _ in range(15):\n",
    "    spk2 = model(input)\n",
    "    result = torch.sum(spk2)\n",
    "    loss = torch.abs(label - result)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJW2GUF0QNJ1"
   },
   "source": [
    "The training process is similar to traditional neural networks. snnTorch also supports the PyTorch optimizer. We can see that the loss is reduced during training. Since we only have one training data, the results don't have any real meaning. This is just a small demo based on the fully connected network structure. You will complete an SNN based on the CNN network structure in the practical assignment problem1 and use it to predict the angle variation of the double pendulum. Good luck with the following work!\n",
    "\n",
    "Also, here are some useful tutorials from snnTorch:\n",
    "\n",
    "https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_2.html\n",
    "\n",
    "https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_3.html\n",
    "\n",
    "https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_5.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
