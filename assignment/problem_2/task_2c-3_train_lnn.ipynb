{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac408b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7619294def77410a539e618e2428f0d5",
     "grade": false,
     "grade_id": "cell-b00828259c8e42e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# RO47019: Intelligent Control Systems Practical Assignment\n",
    "* Period: 2023-2024, Q3\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/500969\n",
    "* Instructor: Cosimo Della Santina (C.DellaSantina@tudelft.nl)\n",
    "* Teaching assistant: Maria de Neves de Fonseca (M.deNevesdeFonseca-1@student.tudelft.nl)\n",
    "* (c) TU Delft, 2024\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Remove `raise NotImplementedError()` afterwards. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names, student numbers, netID, and emails below.\n",
    "STUDENT_1_NAME = \"\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"\"\n",
    "STUDENT_1_NETID = \"\"\n",
    "STUDENT_1_EMAIL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba32571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042927213b84aa368aa3ea72caa4cb60",
     "grade": true,
     "grade_id": "cell-9f148ec62e0de49c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert STUDENT_1_NAME != \"\"\n",
    "assert STUDENT_1_STUDENT_NUMBER != \"\"\n",
    "assert STUDENT_1_NETID != \"\"\n",
    "assert STUDENT_1_EMAIL != \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95c5b11f9ac3896252d342cabb38d867",
     "grade": false,
     "grade_id": "cell-4ea391677951116c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions (also after the course is finished), and do *not* copy solutions from others. By submitting your solutions, you claim that you alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled Q&A hours to ask the TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for you that you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "387d2c60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa1bd6dac9e3191ddb8d246e783d28d8",
     "grade": false,
     "grade_id": "cell-ed88010142fb94bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Task 2c.3 - Train the Lagrangian neural network (4.5p)\n",
    "\n",
    "**Author:** Maximilian Stölzle (M.W.Stolzle@tudelft.nl)\n",
    "\n",
    "This notebook will guide you through the implementation of a Lagrangian neural network. All functions need to be implemented into the `lnn_training.ipynb` notebook. \n",
    "\n",
    "Please enter your answer where requested and remove raise `NotImplementedError()` afterwards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb1c8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47d97cff0897337c3d4b6524a9e21312",
     "grade": false,
     "grade_id": "cell-1ccdb1fbb18aa565",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Training the neural network might be faster on the GPU than on the CPU. Please note that we will not offer any support for installing JAX with GPU support and it is totally up to your discretion to attempt the GPU training. If you are satisfied with training on the CPU, there is no need to modify the next cell. \n",
    "\n",
    "If your computer has access to a NVIDIA GPU and you want to train the neural network on the GPU, please first install JAX with both CPU and GPU support. You can do this by running in the `ics` Conda environment:\n",
    "\n",
    "```bash\n",
    "conda install jax cuda-nvcc -c conda-forge -c nvidia\n",
    "```\n",
    "\n",
    "Next, you should set in the cell below the default JAX device to GPU with `jax_config.update(\"jax_platform_name\", \"gpu\")`. Then, you will need to restart the kernel of the Jupyter notebook by clicking on _Kernel -> Restart_. If everything went well, the following cells should run-through without any errors or warnings. In case you are encountering issues with the installation or if JAX does not find your GPU, please refer to the [JAX README](https://github.com/google/jax#installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b0d68",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0be006dfbc1b07937cce58439b8b18da",
     "grade": false,
     "grade_id": "cell-45575ae57d8395e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from distutils.util import strtobool\n",
    "from jax import config as jax_config\n",
    "import os\n",
    "\n",
    "# define boolean to check if the notebook is run for the purposes of autograding\n",
    "AUTOGRADING = strtobool(os.environ.get(\"AUTOGRADING\", \"false\"))\n",
    "# define tolerances for grading\n",
    "RTOL = float(os.environ.get(\"RTOL\", \"1e-4\"))  # relative tolerance\n",
    "ATOL = float(os.environ.get(\"ATOL\", \"1e-7\"))  # absolute tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c39bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default device to \"cpu\"\n",
    "# for training on the GPU, change to \"gpu\"\n",
    "jax_config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dacc9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09fe2f3835f59331be03d9fe9aeff219",
     "grade": false,
     "grade_id": "cell-d7243ceefb433f94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "jax_config.update(\"jax_enable_x64\", True)  # double precision\n",
    "\n",
    "if AUTOGRADING:\n",
    "    # when autograding, always use CPU\n",
    "    jax_config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "# Reloads the python files outside of this notebook automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import all Python modules\n",
    "import dill\n",
    "from flax.core import FrozenDict\n",
    "from flax.training.train_state import TrainState\n",
    "from functools import partial\n",
    "import jax\n",
    "from jax import random, vmap\n",
    "from jax import numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import optax\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "from jax_double_pendulum.utils import normalize_link_angles\n",
    "\n",
    "from lnn_analysis import (\n",
    "    plot_lnn_training_convergence,\n",
    "    plot_link_angular_acceleration_prediction_error,\n",
    ")\n",
    "\n",
    "# import the learned discrete forward dynamics from lnn.ipynb\n",
    "from ipynb.fs.full.lnn import (\n",
    "    discrete_forward_dynamics,\n",
    "    MassMatrixNN,\n",
    "    PotentialEnergyNN,\n",
    ")\n",
    "\n",
    "# specify directory where to save plot\n",
    "outputs_dir = Path(\"outputs\")\n",
    "outputs_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857757c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d32b8211ff774827a5870dbb4725bb90",
     "grade": false,
     "grade_id": "cell-33032d065a5ba87d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Loading & splitting of the dataset\n",
    "\n",
    "Please implement the splitting of the dataset into a training and a validation set into the function `load_datasets`. We emphasize that the dataset needs to be **randomly** split, as otherwise certain parts of the state-space could only be represented in the validation set and not the training set (and vice-versa). **Hint:** one option for generating this random split is to implement a random permutation of the dataset indices before separating the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1a659",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "844e59caeca3463ae5ee4ed875241fc5",
     "grade": false,
     "grade_id": "cell-62269f573e40bb93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import load_datasets from lnn_training.ipynb\n",
    "from ipynb.fs.full.lnn_training import load_datasets\n",
    "\n",
    "_rng = jax.random.PRNGKey(seed=101)\n",
    "\n",
    "# load the dataset\n",
    "_train_ds, _val_ds = load_datasets(\n",
    "    Path(\"datasets\") / \"dataset_double_pendulum_dynamics.npz\",\n",
    "    _rng,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The training and validation set contain {_train_ds['dt_ss'].shape[0]} \"\n",
    "    f\"and {_val_ds['dt_ss'].shape[0]} samples respectively.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cb2cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "236ff7e8d3a43b6ced7a45469b1d7174",
     "grade": false,
     "grade_id": "cell-38e8620497111fd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Creating the learning rate scheduler (2p)\n",
    "\n",
    "For this assignment, we ask you to implement a cosine decay learning rate scheduler with warmup. Namely, a scheduler will adjust the learning rate within gradient descent optimization as a function of the progress (e.g. steps or epochs) of the training. As already mentioned, this scheduler consists of two main components:\n",
    "\n",
    "1. **Warmup:** During the warmup phase, the learning rate is linearly increased for a specified number of epochs until it reaches a certain base learning rate. The purpose of this warmup phase is to provide stability to the optimization procedure, as the neural network weights are still far away from their optimal values.\n",
    "\n",
    "2. **Cosine decay:** Cosine decay scheduling causes a slow decrease of the learning rate. This is useful to gradually increase the fine-tuning of the weights.\n",
    "\n",
    "Please implement the factor function `create_learning_rate_fn`, which should produce a _learning rate function_. Given the index of the current step, this `learning_rate_fn` should return the applicable learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ddccb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38053a39665d4a490bc29b222101955a",
     "grade": false,
     "grade_id": "cell-323dc77987e26f1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import create_learning_rate_fn from lnn_training.ipynb\n",
    "from ipynb.fs.full.lnn_training import create_learning_rate_fn\n",
    "\n",
    "# plot the learning rate over the training steps for some dummy settings\n",
    "_num_epochs = 20\n",
    "_warmup_epochs = 5\n",
    "_base_lr = 1e-2\n",
    "_steps_per_epoch = 100\n",
    "\n",
    "# call the factory function for the learning rate scheduler\n",
    "_learning_rate_fn = create_learning_rate_fn(\n",
    "    _num_epochs, _steps_per_epoch, _base_lr, _warmup_epochs\n",
    ")\n",
    "\n",
    "# evaluate the learning rate at each step\n",
    "_steps = jnp.arange(0, _num_epochs * _steps_per_epoch)\n",
    "_lrs = vmap(_learning_rate_fn, 0, 0)(_steps)\n",
    "\n",
    "# plot the learning rate vs. training steps\n",
    "_fig = plt.figure(figsize=(6.4, 3.5))\n",
    "plt.plot(_steps, _lrs)\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217744e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ffc84c69ef49c229c01a3801fb78813",
     "grade": true,
     "grade_id": "cell-a8e373d1c2b0bf69",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1fbae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "547c6f65d84a75ced836a19027d36048",
     "grade": false,
     "grade_id": "cell-32994a1facffe1ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Initializing the train states (1p)\n",
    "\n",
    "Next in `initialize_train_states`, we initialize the neural network weights, the optimizer and save both in the Flax `TrainState` object:\n",
    "\n",
    "1. For initializing the neural network weights, you can call the `init` method of the neural network object and as arguments provide a PRNG key and dummy inputs.\n",
    "\n",
    "2. As an optimizer, we use the _AdamW_ optimizer, which is an Adam optimizer with weight decay. Remember to pass your custom `learning_rate_fn` to the `optax.adamw` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a2ef2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a0ca2fc622636a5f9174e0d56d892e9",
     "grade": true,
     "grade_id": "cell-6232941c29efd4bc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n",
    "\n",
    "# import initialize_train_states from lnn_training.ipynb\n",
    "from ipynb.fs.full.lnn_training import initialize_train_states\n",
    "\n",
    "_states = initialize_train_states(_rng, _learning_rate_fn)\n",
    "\n",
    "_mass_matrix_nn_params = _states[\"MassMatrixNN\"].params\n",
    "_potential_energy_nn_params = _states[\"PotentialEnergyNN\"].params\n",
    "\n",
    "print(\"MassMatrixNN params:\\n\", _mass_matrix_nn_params)\n",
    "print(\"PotentialEnergyNN params:\\n\", _potential_energy_nn_params)\n",
    "\n",
    "assert (\n",
    "    type(_mass_matrix_nn_params) == dict\n",
    "), \"The MassMatrixNN params needs to be of type Dict\"\n",
    "assert (\n",
    "    type(_potential_energy_nn_params) == dict\n",
    "), \"The PotentialEnergyNN params needs to be of type Dict\"\n",
    "\n",
    "target_layer_keys = [\"Dense_0\", \"Dense_1\", \"Dense_2\", \"Dense_3\", \"Dense_4\"]\n",
    "grader_points = 0.0\n",
    "\n",
    "if list(_mass_matrix_nn_params.keys()) == target_layer_keys:\n",
    "    grader_points += 0.5\n",
    "else:\n",
    "    print(\n",
    "        \"The MassMatrixNN params are wrongly initialized!\"\n",
    "        \"Currently, the keys of the parameter dictionary are\",\n",
    "        _mass_matrix_nn_params.keys(),\n",
    "    )\n",
    "\n",
    "if list(_potential_energy_nn_params.keys()) == target_layer_keys:\n",
    "    grader_points += 0.5\n",
    "else:\n",
    "    print(\n",
    "        \"The PotentialEnergyNN params are wrongly initialized!\"\n",
    "        \"Currently, the keys of the parameter dictionary are\",\n",
    "        _potential_energy_nn_params.keys(),\n",
    "    )\n",
    "\n",
    "grader_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e5f40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "151b183a684a44f930e39f603e4c64bd",
     "grade": false,
     "grade_id": "cell-ea72d73a29239ff9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Mean Squared Error (1p)\n",
    "\n",
    "Please implement the `mse_loss_fn`, which shall compute the Mean Squared Error (MSE) between an array of predictions and an array of targets (i.e. labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876d4c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b98df1eac0132bc36501d4f6679eecf9",
     "grade": true,
     "grade_id": "cell-7ad89f9813d852dd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n",
    "\n",
    "# import mse_loss_fn from lnn_training.ipynb\n",
    "from ipynb.fs.full.lnn_training import mse_loss_fn\n",
    "\n",
    "_rng1, _rng2 = random.split(_rng)\n",
    "\n",
    "_pred = random.randint(_rng1, shape=(4, 6), minval=0, maxval=10)\n",
    "_target = random.randint(_rng2, shape=(4, 6), minval=0, maxval=10)\n",
    "print(\"Prediction:\\n\", _pred)\n",
    "print(\"Target:\\n\", _target)\n",
    "\n",
    "_mse = mse_loss_fn(_pred, _target)\n",
    "print(\"MSE:\", _mse)\n",
    "\n",
    "# check that all dimensions have been reduced\n",
    "assert _mse.shape == ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427788e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a02656184209ed1fda7dfba165029668",
     "grade": false,
     "grade_id": "cell-1ae302a78835cad3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Vectorization of the discrete-time forward dynamics (0.5p)\n",
    "\n",
    "For training the neural network, we need to predict the link angles and the angular velocities at the next timestep for a **batch** of data. Although the function `discrete_forward_dynamics` in the `lnn.ipynb` notebook is only formulated for a single datapoint, we can use `jax.vmap` for vectorization. Please implement the factory function `make_vectorized_discrete_forward_dynamics`, which shall return the callable `vectorized_discrete_forward_dynamics_fn` with the following signature:\n",
    "\n",
    "```python\n",
    "th_next_pred_ss, th_d_next_pred_ss, th_dd_ss = vectorized_discrete_forward_dynamics_fn(\n",
    "    mass_matrix_nn_params,\n",
    "    potential_energy_nn_params,\n",
    "    dt_ss,\n",
    "    th_curr_ss,\n",
    "    th_d_curr_ss,\n",
    "    tau_ss,\n",
    ")\n",
    "```\n",
    "where `dt_ss` has the shape (N, ), `th_curr_ss` has the shape (N, 2), `th_d_curr_ss` has the shape (N, 2),\n",
    "`tau_ss` has the shape (N, 2), `th_next_pred_ss` has the shape (N, 2), `th_d_next_pred_ss` has the shape (N, 2)\n",
    "`th_dd_ss` has the shape (N, 2). N is the number of samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df940716",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46d10e621343d7556dd2950097569db1",
     "grade": true,
     "grade_id": "cell-8e5b9740ff4cfa2c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n",
    "\n",
    "# import make_vectorized_discrete_forward_dynamics from lnn_training.ipynb\n",
    "from ipynb.fs.full.lnn_training import make_vectorized_discrete_forward_dynamics\n",
    "\n",
    "# call the factory function for `vectorized_discrete_forward_dynamics_fn`\n",
    "vectorized_discrete_forward_dynamics_fn = make_vectorized_discrete_forward_dynamics()\n",
    "\n",
    "_N = 10\n",
    "_dt_ss = 1e-2 * jnp.ones((_N,))\n",
    "_th_curr_ss = jnp.zeros((_N, 2))\n",
    "_th_curr_ss = _th_curr_ss.at[:, 0].set(jnp.linspace(-jnp.pi, jnp.pi, _N))\n",
    "_th_curr_ss = _th_curr_ss.at[:, 1].set(jnp.linspace(jnp.pi, -jnp.pi, _N))\n",
    "_th_d_curr_ss = jnp.pi * jnp.ones((_N, 2))\n",
    "_tau_ss = jnp.zeros((_N, 2))\n",
    "\n",
    "# neural network params\n",
    "_mass_matrix_nn_params = _states[\"MassMatrixNN\"].params\n",
    "_potential_energy_nn_params = _states[\"PotentialEnergyNN\"].params\n",
    "\n",
    "(\n",
    "    _th_next_pred_ss,\n",
    "    _th_d_next_pred_ss,\n",
    "    _th_dd_ss,\n",
    ") = vectorized_discrete_forward_dynamics_fn(\n",
    "    _mass_matrix_nn_params,\n",
    "    _potential_energy_nn_params,\n",
    "    _dt_ss,\n",
    "    _th_curr_ss,\n",
    "    _th_d_curr_ss,\n",
    "    _tau_ss,\n",
    ")\n",
    "\n",
    "print(\"Computed th_next_pred_ss:\\n\", _th_next_pred_ss)\n",
    "print(\"Computed th_d_next_pred_ss:\\n\", _th_d_next_pred_ss)\n",
    "print(\"Computed th_dd_ss:\\n\", _th_dd_ss)\n",
    "\n",
    "assert _th_next_pred_ss.shape == (_N, 2)\n",
    "assert _th_d_next_pred_ss.shape == (_N, 2)\n",
    "assert _th_dd_ss.shape == (_N, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381a637",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc4a884f7b86f08500052ce0e50ed282",
     "grade": false,
     "grade_id": "cell-9090cde50e4f247b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training and evaluation\n",
    "\n",
    "Implement the execution of one training and evaluation step (i.e. processing of one minibatch) into the `train_step` and `eval_step` functions respectively. The `train_epoch` and the `eval_model` functions are already complete, but it is still worth for you to investigate their purpose and functionality. \n",
    "\n",
    "Below, we give you an overview of the necessary steps involved in one training epoch:\n",
    "\n",
    "- We first randomly shuffle the order of the samples in the training set. Then, we divide the training set into many minibatches (all done in `train_epoch`).\n",
    "\n",
    "- For each batch, we send the inputs through the LNN and evaluate the outputs of the LNN againts the labels for that batch (done in `train_step`).\n",
    "\n",
    "- The MSE loss function called in `loss_fn` will provide us with a scalar loss metric of how well the LNN prediction matches with the target.\n",
    "\n",
    "- In the `train_step`, we trace the gradients of the loss with respect to the neural network parameters backwards through the LNN.\n",
    "\n",
    "- Then (again in `train_step`), `AdamW` will optimize the parameters of both neural networks using the computed gradients and considering the current learning rate. The current learning rate is provided by the learning rate scheduler, which is evaluated at the current step index (`TrainState.step`).\n",
    "\n",
    "- Finally, an epoch is considered to be completed, when all samples of the training set have been processed **exactly** once.\n",
    "\n",
    "**Hint:** to access both the loss and the gradient of the loss with respect to the network parameters, apply the `jax.value_and_grad` on the `loss_fn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450fa5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d4a36de11b58c79686579156796994e",
     "grade": false,
     "grade_id": "cell-4188d90aeca08c59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training loop: putting everything together\n",
    "\n",
    "Please implement the full training loop into the function `run_lnn_training` in the `lnn_training.ipynb` notebook. \n",
    "\n",
    "In this training loop, we will repeat `train_epoch` multiple times (i.e. repeat the neural network parameter optimization for multiple epochs). For each epoch, we run `train_epoch` with a newly (randomly) shuffled training set, evaluate the freshly optimized neural network parameters on the validation set and save all data as a history, such that we can later track the training progress and identify the epoch with the lowest loss on the validation set.\n",
    "\n",
    "\n",
    "You will need to implement the following items in `run_lnn_training`: first, you need to initialize the learning rate scheduler. Then, for each epoch of the training loop, you should:\n",
    "\n",
    "1. Split the PRNG key for shuffling the training set for the current epoch.\n",
    "\n",
    "2. Call the `train_epoch` function, which will return the training loss, some metrics on the training set and the freshly optimized training states containing the neural network parameters.\n",
    "\n",
    "3. Call the `eval_model` function, which will return the validation loss and the validation metrics for the current training state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39989911",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32e2fd46f76c019fda1bc69954dac9a8",
     "grade": false,
     "grade_id": "cell-b39fec3d11f4e722",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Executing the training loop\n",
    "\n",
    "Now, you can go ahead and run the training. We already provide you with a base set of hyperparameters, which in combination with a correct implementation and a suitable dataset should lead to decent performance. However, you are free to tune the hyperparameters yourself further to improve performance.\n",
    "\n",
    "As the training could / should take longer, we advise you to take a ☕ break or to start already with one of the other tasks in the meantime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "# these default values should already be working decently\n",
    "# if you want to improve performance further, you can try fine-tuning these hyperparameters\n",
    "\n",
    "# how many epochs to train for\n",
    "num_epochs = 250\n",
    "# base learning rate after warmup. Then, cosine decay will decrease the learning rate again\n",
    "base_lr = 7e-4\n",
    "# the learning rate will be linearly increase for `warmup_epochs` before reaching the `base_lr`\n",
    "warmup_epochs = 10\n",
    "# number of datapoints in one batch\n",
    "batch_size = 250\n",
    "# regularization / penalty on the magnitude of the NN weights\n",
    "weight_decay = 0e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0eb120",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ceb4d62680d53de04c4f451d45a7fee2",
     "grade": false,
     "grade_id": "cell-b6b00c6258dcd124",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import run_lnn_training from lnn_training.ipynb\n",
    "from ipynb.fs.full.lnn_training import run_lnn_training\n",
    "\n",
    "# initialize the PRNG key at seed=0\n",
    "rng = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# split of PRNG keys\n",
    "# the 1st is used for training,\n",
    "# the 2nd to split the dataset\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# load the dataset\n",
    "train_ds, val_ds = load_datasets(\n",
    "    Path(\"datasets\") / \"dataset_double_pendulum_dynamics.npz\",\n",
    "    dataset_split_rng,\n",
    ")\n",
    "\n",
    "if not AUTOGRADING:\n",
    "    # run the training calling `run_lnn_training`\n",
    "    val_loss_history = []\n",
    "    train_metrics_history = []\n",
    "    val_metrics_history = []\n",
    "    states_history = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb06db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09db20cf1857d0f0640a53290e1bb9b9",
     "grade": false,
     "grade_id": "cell-344cdd065c6225fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# visualization of the validation loss vs. epochs\n",
    "if not AUTOGRADING:\n",
    "    plot_lnn_training_convergence(\n",
    "        val_loss_history,\n",
    "        train_metrics_history,\n",
    "        val_metrics_history,\n",
    "        show=True,\n",
    "        filepath=str(outputs_dir / \"task_2c-3_lnn_training_convergence.pdf\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855f63a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b986fbbd2c3b4396ba23eba6b3a7abd",
     "grade": false,
     "grade_id": "cell-226c1d240009383f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After the training loop is complete, we evaluate the performance of the neural network. It is important to emphasize that the best performance on the validation set is not necessarily always achieved during the last epoch of the training (e.g. overfitting, noise etc.). Therefore, we track the validation losses throughout the entire training, which allows us to identify the best epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef99c1d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b7a89bba7d0ff456dea275883b2ade3",
     "grade": false,
     "grade_id": "cell-355eb31771ab4317",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    # extract the best epoch with the lowest validation loss\n",
    "    best_epoch = 0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    best_val_loss = val_loss_history[best_epoch]\n",
    "    best_val_metrics = val_metrics_history[best_epoch]\n",
    "    best_states = states_history[best_epoch]\n",
    "\n",
    "    print(\n",
    "        \"Found lowest validation loss of\",\n",
    "        best_val_loss,\n",
    "        \"at epoch\",\n",
    "        best_epoch,\n",
    "        \"with metrics\\n\",\n",
    "        best_val_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b5742",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dabf67072871d1ea7ec6d261a5ee453",
     "grade": false,
     "grade_id": "cell-cecc3c8d3b947b9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we save the neural network parameters, which are stored in the variable `best_states`, to the path `statedicts/task_2c_lagrangian_nn_params.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8347e69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e076f9d3aeadf7cafba7f5c28eb91d35",
     "grade": false,
     "grade_id": "cell-3690d02e2ab4803f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    # folder where to save neural network parameters\n",
    "    statedict_dir = Path(\"statedicts\")\n",
    "    statedict_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Save the neural network parameters\n",
    "    with open(str(statedict_dir / \"task_2c_lagrangian_nn_params.pkl\"), \"wb\") as f:\n",
    "        nn_params = {\n",
    "            \"MassMatrixNN\": best_states[\"MassMatrixNN\"].params,\n",
    "            \"PotentialEnergyNN\": best_states[\"PotentialEnergyNN\"].params,\n",
    "        }\n",
    "        dill.dump(nn_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190ca65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9d651c87c5052815a07981b63d7c853",
     "grade": false,
     "grade_id": "cell-553d9922461ce845",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualizing the prediction performance\n",
    "\n",
    "We predict with the trained LNN the discrete-time state transitions on the validation set and visualize the result in a polar plot with the link angular velocity $\\dot{\\theta}$ on the radial axis and the color visualizing the absolute value of the link angular acceleration prediction error $| \\hat{\\ddot{\\theta}} - \\ddot{\\theta} |$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7274fa2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd2656610f45c17e9d3b518218c54d3",
     "grade": false,
     "grade_id": "cell-a71e720527844945",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    # plot the prediction error of the best model\n",
    "    # Compute the estimated link angles and velocities\n",
    "    th_next_pred, th_d_next_pred, th_dd_pred = vectorized_discrete_forward_dynamics_fn(\n",
    "        best_states[\"MassMatrixNN\"].params,\n",
    "        best_states[\"PotentialEnergyNN\"].params,\n",
    "        val_ds[\"dt_ss\"],\n",
    "        val_ds[\"th_curr_ss\"],\n",
    "        val_ds[\"th_d_curr_ss\"],\n",
    "        val_ds[\"tau_ss\"],\n",
    "    )\n",
    "    preds = {\"th_next_ss\": th_next_pred, \"th_d_next_ss\": th_d_next_pred}\n",
    "\n",
    "    # plot the predictions\n",
    "    plot_link_angular_acceleration_prediction_error(\n",
    "        val_ds,\n",
    "        preds,\n",
    "        filepath=str(\n",
    "            outputs_dir / \"task_2c-3_link_angular_accelerations_prediction_error.pdf\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebb02a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
